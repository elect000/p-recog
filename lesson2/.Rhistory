# output layer
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
# preparing train/test data
train.array <- train.x
dim(train.array) <- c(28, 28, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))
# preparing training
mx.set.seed(0)
devices <- mx.cpu()
tic <- proc.time()
print(proc.time() - tic)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
pred.label <- max.col(t(preds)) -1
mean(test_org[,1] == pred.label)
print(proc.time() - tic)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
preds <- predict(model.CNNtanhDrop, test, ctx=devices)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
pred.label <- max.col(t(preds)) -1
mean(test_org[,1] == pred.label)
# [25] Train-accuracy=0.969400004148483
# [26] Train-accuracy=0.970800008773804
# [27] Train-accuracy=0.971400009393692
# [28] Train-accuracy=0.966400008201599
# [29] Train-accuracy=0.968200007677078
# [30] Train-accuracy=0.972400006055832
# user  system elapsed
# 543.69  238.85  157.29
# [1] 0.98
# ---------------------------------------------------------
dropout = TRUE
activate_fn = "relu"
data <- mx.symbol.Variable("data")
# hidden layer 1
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5, 5), num_filter=20)
tanh1 <- mx.symbol.Activation(data=conv1, act_type=activate_fn)
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
if (dropout) {
drop1 <- mx.symbol.Dropout(data=pool1, p=0.5)
} else {
drop1 <- pool1
}
# hidden layer 2
conv2 <- mx.symbol.Convolution(data=drop1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type=activate_fn)
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
if (dropout) {
drop2 <- mx.symbol.Dropout(data=pool2, p=0.5)
} else {
drop2 <- pool2
}
# fully connected layer 1
flatten <- mx.symbol.Flatten(data=drop2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type=activate_fn)
if (dropout) {
drop3 <- mx.symbol.Dropout(data=tanh3, p=0.5)
} else {
drop3 <- tanh3
}
# fully connected layer 2
fc2 <- mx.symbol.FullyConnected(data=drop3, num_hidden=10)
# output layer
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
# preparing train/test data
train.array <- train.x
dim(train.array) <- c(28, 28, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))
# preparing training
mx.set.seed(0)
devices <- mx.cpu()
tic <- proc.time()
# training model
model.CNNtanhDrop <- mx.model.FeedForward.create(lenet, X=train.array,
y=train.y, ctx=devices, num.round = 30, array.batch.size = 100,
learning.rate=0.05, momentum=0.9, wd=0.000001,
eval.metric=mx.metric.accuracy,
batch.end.callback = mx.callback.log.train.metric(100))
print(proc.time() - tic)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
pred.label <- max.col(t(preds)) -1
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
preds <- predict(model.CNNtanhDrop, test, ctx=devices)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
# preparing train/test data
train.array <- train.x
dim(train.array) <- c(28, 28, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
print(proc.time() - tic)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
preds <- predict(model, test.array, ctx=devices)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
dim(test.array)
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))
# create dataset
train <- read.csv("data/short_prac_train.csv", header = TRUE)
test <- read.csv("data/short_prac_test.csv", header = TRUE)
train <- data.matrix(train)
test <- data.matrix(test)
train.x <- train[,-1]
train.y <- train[,1]
test_org <- test
test <- test[,-1]
train.x <- t(train.x/255) # [0, 255] -> [0, 1]
test <- t(test/255)
table(train.y)
# input layer
data <- mx.symbol.Variable("data")
# hidden layer 1
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5, 5), num_filter=20)
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
drop1 <- mx.symbol.Dropout(data=pool1, p=0.5)
# hidden layer 2
conv2 <- mx.symbol.Convolution(data=drop1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
drop2 <- mx.symbol.Dropout(data=pool2, p=0.5)
# fully connected layer 1
flatten <- mx.symbol.Flatten(data=drop2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
drop3 <- mx.symbol.Dropout(data=tanh3, p=0.5)
# fully connected layer 2
fc2 <- mx.symbol.FullyConnected(data=drop3, num_hidden=10)
# output layer
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
# preparing train/test data
train.array <- train.x
dim(train.array) <- c(28, 28, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))
dim(test.array)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
pred.label <- max.col(t(preds)) -1
mean(test_org[,1] == pred.label)
print(proc.time() - tic)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
pred.label <- max.col(t(preds)) -1
mean(test_org[,1] == pred.label)
test_org[,-1]
pred.label
test_org[,-1]
test_org
test
dim(test)
dim(test_org)
dim(test_org[,-1])
dim(pred.label)
length(pred.label)
library(MASS)
# R 3.5.1 で実行確認
# import libraries
library(nnet)
# create dataset
train <- read.csv("data/short_prac_train.csv", header = TRUE)
test <- read.csv("data/short_prac_test.csv", header = TRUE)
train <- data.matrix(train) test <- data.matrix(test)
train.x <- train[,-1]
train.y <- train[,1]
test <- test[,-1]
train.x <- t(train.x/255) # [0, 255] -> [0, 1]
test <- t(test/255)
# check image
image(x=seq(1:28),y=seq(1:28), matrix(train.x[,4], 28, 28)[, 28:1],
col = gray(0:255/255))
# sample
# network settings
data <- mx.symbol.Variable("data")
fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=128)
act1 <- mx.symbol.Activation(fc1, name="relu1", act_type="relu")
fc2 <- mx.symbol.FullyConnected(act1, name="fc2", num_hidden=64)
table(train.y)
test_org <- test
library(mxnet)
length(test_org[,-1])
length(test_org[,1])
sum(diag(table(test_org[,1], pred.label))) / 1000
# R 3.5.1 で実行確認
# import libraries
library(nnet)
library(MASS)
library(mxnet)
# create dataset
train <- read.csv("data/short_prac_train.csv", header = TRUE)
test <- read.csv("data/short_prac_test.csv", header = TRUE)
test <- data.matrix(test)
train.x <- train[,-1]
train.y <- train[,1]
test_org <- test
test <- t(test/255)
table(train.y)
# input layer
data <- mx.symbol.Variable("data")
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
drop1 <- mx.symbol.Dropout(data=pool1, p=0.5)
# hidden layer 2
conv2 <- mx.symbol.Convolution(data=drop1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
drop2 <- mx.symbol.Dropout(data=pool2, p=0.5)
# fully connected layer 1
flatten <- mx.symbol.Flatten(data=drop2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
drop3 <- mx.symbol.Dropout(data=tanh3, p=0.5)
# hidden layer 1
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5, 5), num_filter=20)
train <- data.matrix(train)
test <- test[,-1]
train.x <- t(train.x/255) # [0, 255] -> [0, 1]
# fully connected layer 2
fc2 <- mx.symbol.FullyConnected(data=drop3, num_hidden=10)
sum(diag(table(test_org[,1], pred.label))) / 1000
# R 3.5.1 で実行確認
# import libraries
library(nnet)
library(MASS)
library(mxnet)
# create dataset
train <- read.csv("data/short_prac_train.csv", header = TRUE)
train <- data.matrix(train)
test <- data.matrix(test)
train.y <- train[,1]
test_org <- test
test <- test[,-1]
train.x <- t(train.x/255) # [0, 255] -> [0, 1]
table(train.y)
# input layer
data <- mx.symbol.Variable("data")
# hidden layer 1
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5, 5), num_filter=20)
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
drop1 <- mx.symbol.Dropout(data=pool1, p=0.5)
# hidden layer 2
conv2 <- mx.symbol.Convolution(data=drop1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
drop2 <- mx.symbol.Dropout(data=pool2, p=0.5)
# fully connected layer 1
flatten <- mx.symbol.Flatten(data=drop2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
drop3 <- mx.symbol.Dropout(data=tanh3, p=0.5)
# output layer
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
# preparing train/test data
train.array <- train.x
dim(train.array) <- c(28, 28, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))
# preparing training
mx.set.seed(0)
devices <- mx.cpu()
tic <- proc.time()
# training model
model.CNNtanhDrop <- mx.model.FeedForward.create(lenet, X=train.array,
y=train.y, ctx=devices, num.round = 30, array.batch.size = 100,
learning.rate=0.05, momentum=0.9, wd=0.000001,
eval.metric=mx.metric.accuracy,
batch.end.callback = mx.callback.log.train.metric(100))
print(proc.time() - tic)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
pred.label <- max.col(t(preds)) -1
training_mnist_cnn = function(dropout, activate_fn) {
# input layer
data <- mx.symbol.Variable("data")
# hidden layer 1
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5, 5), num_filter=20)
tanh1 <- mx.symbol.Activation(data=conv1, act_type=activate_fn)
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
if (dropout) {
drop1 <- mx.symbol.Dropout(data=pool1, p=0.5)
} else {
drop1 <- pool1
}
# hidden layer 2
conv2 <- mx.symbol.Convolution(data=drop1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type=activate_fn)
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
if (dropout) {
drop2 <- mx.symbol.Dropout(data=pool2, p=0.5)
} else {
drop2 <- pool2
}
# fully connected layer 1
flatten <- mx.symbol.Flatten(data=drop2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type=activate_fn)
if (dropout) {
drop3 <- mx.symbol.Dropout(data=tanh3, p=0.5)
} else {
drop3 <- tanh3
}
# fully connected layer 2
fc2 <- mx.symbol.FullyConnected(data=drop3, num_hidden=10)
# output layer
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
# preparing train/test data
train.array <- train.x
dim(train.array) <- c(28, 28, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))
# preparing training
mx.set.seed(0)
devices <- mx.cpu()
tic <- proc.time()
# training model
model.CNNtanhDrop <- mx.model.FeedForward.create(lenet, X=train.array,
y=train.y, ctx=devices, num.round = 30, array.batch.size = 100,
learning.rate=0.05, momentum=0.9, wd=0.000001,
eval.metric=mx.metric.accuracy,
batch.end.callback = mx.callback.log.train.metric(100))
print(proc.time() - tic)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
pred.label <- max.col(t(preds)) -1
sum(diag(table(test_org[,1], pred.label))) / 1000
}
# 3)
training_mnist_cnn(FALSE, "tanh")
# [27] Train-accuracy=1
# [28] Train-accuracy=1
# [29] Train-accuracy=1
# [30] Train-accuracy=1
# user  system elapsed
# 549.00  251.10  163.95
# [1] 0.982
# --------------------------------------------------------------------
# comment: over fitting
# 4)
training_mnist_cnn(FALSE, "relu")
# [26] Train-accuracy=0.999800000190735
# [27] Train-accuracy=1
# [28] Train-accuracy=1
# [29] Train-accuracy=1
# [30] Train-accuracy=1
# user  system elapsed
# 513.99  243.98  154.81
# [1] 0.982
# --------------------------------------------------------------------
# 5)
training_mnist_cnn(TRUE, "relu")
# [1] Train-accuracy=0.0949999997764826
# [1] Train-accuracy=0.0949999997764826
# [2] Train-accuracy=0.0893999997526407
sum(diag(table(test_org[,1], pred.label))) / 1000
# fully connected layer 2
fc2 <- mx.symbol.FullyConnected(data=drop3, num_hidden=10)
test <- read.csv("data/short_prac_test.csv", header = TRUE)
test <- t(test/255)
train.x <- train[,-1]
# R 3.5.1 で実行確認
# import libraries
library(nnet)
library(MASS)
library(mxnet)
# create dataset
train <- read.csv("data/short_prac_train.csv", header = TRUE)
test <- read.csv("data/short_prac_test.csv", header = TRUE)
train <- data.matrix(train)
test <- data.matrix(test)
train.x <- train[,-1]
train.y <- train[,1]
test_org <- test
test <- test[,-1]
train.x <- t(train.x/255) # [0, 255] -> [0, 1]
test <- t(test/255)
table(train.y)
# input layer
data <- mx.symbol.Variable("data")
# hidden layer 1
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5, 5), num_filter=20)
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
drop1 <- mx.symbol.Dropout(data=pool1, p=0.5)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
drop2 <- mx.symbol.Dropout(data=pool2, p=0.5)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
drop3 <- mx.symbol.Dropout(data=tanh3, p=0.5)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
# fully connected layer 1
flatten <- mx.symbol.Flatten(data=drop2)
# hidden layer 2
conv2 <- mx.symbol.Convolution(data=drop1, kernel=c(5,5), num_filter=50)
# fully connected layer 2
fc2 <- mx.symbol.FullyConnected(data=drop3, num_hidden=10)
# output layer
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
# preparing train/test data
train.array <- train.x
dim(train.array) <- c(28, 28, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))
# preparing training
mx.set.seed(0)
devices <- mx.cpu()
tic <- proc.time()
# training model
model.CNNtanhDrop <- mx.model.FeedForward.create(lenet, X=train.array,
y=train.y, ctx=devices, num.round = 30, array.batch.size = 100,
learning.rate=0.05, momentum=0.9, wd=0.000001,
eval.metric=mx.metric.accuracy,
batch.end.callback = mx.callback.log.train.metric(100))
print(proc.time() - tic)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
pred.label <- max.col(t(preds)) -1
sum(diag(table(test_org[,1], pred.label))) / 1000
training_mnist_cnn = function(dropout, activate_fn) {
# input layer
data <- mx.symbol.Variable("data")
# hidden layer 1
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5, 5), num_filter=20)
tanh1 <- mx.symbol.Activation(data=conv1, act_type=activate_fn)
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
if (dropout) {
drop1 <- mx.symbol.Dropout(data=pool1, p=0.5)
} else {
drop1 <- pool1
}
# hidden layer 2
conv2 <- mx.symbol.Convolution(data=drop1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type=activate_fn)
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max", kernel=c(2, 2),
stride=c(2, 2))
if (dropout) {
drop2 <- mx.symbol.Dropout(data=pool2, p=0.5)
} else {
drop2 <- pool2
}
# fully connected layer 1
flatten <- mx.symbol.Flatten(data=drop2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type=activate_fn)
if (dropout) {
drop3 <- mx.symbol.Dropout(data=tanh3, p=0.5)
} else {
drop3 <- tanh3
}
# fully connected layer 2
fc2 <- mx.symbol.FullyConnected(data=drop3, num_hidden=10)
# output layer
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
# preparing train/test data
train.array <- train.x
dim(train.array) <- c(28, 28, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))
# preparing training
mx.set.seed(0)
devices <- mx.cpu()
tic <- proc.time()
# training model
model.CNNtanhDrop <- mx.model.FeedForward.create(lenet, X=train.array,
y=train.y, ctx=devices, num.round = 30, array.batch.size = 100,
learning.rate=0.05, momentum=0.9, wd=0.000001,
eval.metric=mx.metric.accuracy,
batch.end.callback = mx.callback.log.train.metric(100))
print(proc.time() - tic)
preds <- predict(model.CNNtanhDrop, test.array, ctx=devices)
pred.label <- max.col(t(preds)) -1
sum(diag(table(test_org[,1], pred.label))) / 1000
}
# 3)
training_mnist_cnn(FALSE, "tanh")
# [27] Train-accuracy=1
# [28] Train-accuracy=1
# [29] Train-accuracy=1
# [30] Train-accuracy=1
# user  system elapsed
# 549.00  251.10  163.95
# [1] 0.982
# --------------------------------------------------------------------
# comment: over fitting
# 4)
training_mnist_cnn(FALSE, "relu")
# [26] Train-accuracy=0.999800000190735
# [27] Train-accuracy=1
# [28] Train-accuracy=1
# [29] Train-accuracy=1
# [30] Train-accuracy=1
# user  system elapsed
# 513.99  243.98  154.81
# [1] 0.982
# --------------------------------------------------------------------
# 5)
training_mnist_cnn(TRUE, "relu")
